<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Practical machine learning by agomezlo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Practical machine learning</h1>
      <h2 class="project-tagline">Coursera. Project write up (October 2015)</h2>
      <a href="https://github.com/agomezlo/Practical-Machine-Learning" class="btn">View on GitHub</a>
      <a href="https://github.com/agomezlo/Practical-Machine-Learning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/agomezlo/Practical-Machine-Learning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p><strong># Practice Machine Learning Writeup</strong></p>

<h2>
<a id="loading-the-data" class="anchor" href="#loading-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading the data</h2>

<p>Unuseful values as "","NA","#DIV/0!" are eliminated from the training data.</p>

<pre><code>training&lt;- read.csv("~/Desktop/.../pml-training.csv", na.strings=c("","NA","#DIV/0!"))
testing&lt;- read.csv("~/Desktop/.../pml-testing.csv")
</code></pre>

<h2>
<a id="select-useful-features" class="anchor" href="#select-useful-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Select useful features</h2>

<p>Detecting the features, we may remove unuseful features for the analysis ("X","raw_timestamp_part_1", etc.)     </p>

<pre><code>user&lt;-grep("user",names(training), value=FALSE) 
belt&lt;-grep("belt",names(training), value=FALSE) 
arm&lt;-grep("[^fore]arm",names(training), value=FALSE)
forearm&lt;-grep("forearm",names(training), value=FALSE)
dumbbell&lt;-grep("dumbbell",names(training), value=FALSE)
classe&lt;-grep("classe",names(training), value=FALSE)

training&lt;-training[,c(user,belt, arm, forearm, dumbbell, classe)]
testing&lt;-testing[,c(user,belt, arm, forearm, dumbbell, classe)]
</code></pre>

<p>It is obtained 54 features in the training and test set.</p>

<pre><code>dim(training)
[1] 19622    54    
dim(testing)
[1] 20 54 
</code></pre>

<h2>
<a id="cleaning-the-data" class="anchor" href="#cleaning-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning the data</h2>

<p>Removing variables which contain just NA vales.</p>

<pre><code>training&lt;-training[,apply(apply(training,2,is.na),2,sum)==0]
testing&lt;-testing[,apply(apply(testing,2,is.na),2,sum)==0]
</code></pre>

<h2>
<a id="building-some-models-getting-the-best-accuracy" class="anchor" href="#building-some-models-getting-the-best-accuracy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building some models: getting the best accuracy.</h2>

<pre><code>mod1 &lt;- train(classe ~.,method="lda",data=train)
pred1&lt;-predict(mod1,test)
confusionMatrix(pred1, test$classe) # Accuracy : 0.7276 

mod2 &lt;- train(classe ~.,method="svmLinear",data=train) # support vector machine
pred2&lt;-predict(mod2,test)
confusionMatrix(pred2, test$classe) # Accuracy : 0.8034

mod3 &lt;- train(classe ~.,method="gbm",data=train) # boosting
pred3&lt;-predict(mod3,test)
confusionMatrix(pred3, test$classe) # Accuracy : 0.9601 

mod4 &lt;- train(classe ~.,method="pcaNNet",data=train) # Neural Networks with Feature Extraction
pred4&lt;-predict(mod4,test)
confusionMatrix(pred4, test$classe) # Accuracy : 0.6348

mod5 &lt;- train(classe ~.,method="rpart",data=train) # model based tree
pred5&lt;-predict(mod5,test)
confusionMatrix(pred5, test$classe) # Accuracy : 0.4748 

mod6 &lt;- train(classe ~.,method="rf",data=train, trControl=trainControl(method="cv"), number=3) # RF
pred6&lt;-predict(mod6,test)
confusionMatrix(pred6, test$classe) # Accuracy : 0.9925  
</code></pre>

<h2>
<a id="model-which-combine-predictors" class="anchor" href="#model-which-combine-predictors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model which combine predictors.</h2>

<pre><code>predDF &lt;- data.frame(pred1,pred2,pred3,pred4,pred5,pred6, classe=test$classe)
combModFit &lt;- train(classe ~.,method="gbm",data=predDF) # the new model
combPred &lt;- predict(combModFit,newdata=predDF)
confusionMatrix(combPred, test$classe) # Accuracy : 0.9925 
</code></pre>

<p>As can be appreciated, the accuracy obtained after combining some models does not improve the accuracy obtained using Random Forest. That is the reason the model 6, which use Randon Forest, will be used instead of the combination of models.</p>

<h2>
<a id="final-prediction-on-the-test-set" class="anchor" href="#final-prediction-on-the-test-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final prediction on the test set</h2>

<pre><code>pred6&lt;-predict(mod6,test)

confusionMatrix(pred6, test$classe)
Confusion Matrix and Statistics

      Reference
Prediction    A    B    C    D    E
         A 1673   12    0    0    0
         B    1 1126    2    0    1
         C    0    1 1019   16    3
         D    0    0    5  948    3
         E    0    0    0    0 1075

Overall Statistics

               Accuracy : 0.9925        
                 95% CI : (0.99, 0.9946)
    No Information Rate : 0.2845        
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     

                  Kappa : 0.9905        
 Mcnemar's Test P-Value : NA            

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9994   0.9886   0.9932   0.9834   0.9935
Specificity            0.9972   0.9992   0.9959   0.9984   1.0000
Pos Pred Value         0.9929   0.9965   0.9808   0.9916   1.0000
Neg Pred Value         0.9998   0.9973   0.9986   0.9968   0.9985
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2843   0.1913   0.1732   0.1611   0.1827
Detection Prevalence   0.2863   0.1920   0.1766   0.1624   0.1827
Balanced Accuracy      0.9983   0.9939   0.9945   0.9909   0.9968
</code></pre>

<h2>
<a id="final-results" class="anchor" href="#final-results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final results</h2>

<pre><code>predict(mod6,testing) 
B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/agomezlo/Practical-Machine-Learning">Practical machine learning</a> is maintained by <a href="https://github.com/agomezlo">agomezlo</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
