{"name":"Pract mach learning","tagline":"Coursera. Practical Machine Learning. Project write up (October 2015)","body":"**# Practice Machine Learning Writeup**\r\n\r\n## Loading the data\r\n\r\nUnuseful values as \"\",\"NA\",\"#DIV/0!\" are eliminated from the training data.\r\n\r\n    training<- read.csv(\"~/Desktop/.../pml-training.csv\", na.strings=c(\"\",\"NA\",\"#DIV/0!\"))\r\n    testing<- read.csv(\"~/Desktop/.../pml-testing.csv\")\r\n\r\n## Select useful features\r\n\r\nDetecting the features, we may remove unuseful features for the analysis (\"X\",\"raw_timestamp_part_1\", etc.)     \r\n\r\n    user<-grep(\"user\",names(training), value=FALSE) \r\n    belt<-grep(\"belt\",names(training), value=FALSE) \r\n    arm<-grep(\"[^fore]arm\",names(training), value=FALSE)\r\n    forearm<-grep(\"forearm\",names(training), value=FALSE)\r\n    dumbbell<-grep(\"dumbbell\",names(training), value=FALSE)\r\n    classe<-grep(\"classe\",names(training), value=FALSE)\r\n\r\n    training<-training[,c(user,belt, arm, forearm, dumbbell, classe)]\r\n    testing<-testing[,c(user,belt, arm, forearm, dumbbell, classe)]\r\n\r\nIt is obtained 54 features in the training and test set.\r\n\r\n    dim(training)\r\n    [1] 19622    54    \r\n    dim(testing)\r\n    [1] 20 54 \r\n\r\n## Cleaning the data\r\nRemoving variables which contain just NA vales.\r\n\r\n    training<-training[,apply(apply(training,2,is.na),2,sum)==0]\r\n    testing<-testing[,apply(apply(testing,2,is.na),2,sum)==0]\r\n\r\n## Building some models: getting the best accuracy.\r\n\r\n    mod1 <- train(classe ~.,method=\"lda\",data=train)\r\n    pred1<-predict(mod1,test)\r\n    confusionMatrix(pred1, test$classe) # Accuracy : 0.7276 \r\n\r\n    mod2 <- train(classe ~.,method=\"svmLinear\",data=train) # support vector machine\r\n    pred2<-predict(mod2,test)\r\n    confusionMatrix(pred2, test$classe) # Accuracy : 0.8034\r\n\r\n    mod3 <- train(classe ~.,method=\"gbm\",data=train) # boosting\r\n    pred3<-predict(mod3,test)\r\n    confusionMatrix(pred3, test$classe) # Accuracy : 0.9601 \r\n\r\n    mod4 <- train(classe ~.,method=\"pcaNNet\",data=train) # Neural Networks with Feature Extraction\r\n    pred4<-predict(mod4,test)\r\n    confusionMatrix(pred4, test$classe) # Accuracy : 0.6348\r\n\r\n    mod5 <- train(classe ~.,method=\"rpart\",data=train) # model based tree\r\n    pred5<-predict(mod5,test)\r\n    confusionMatrix(pred5, test$classe) # Accuracy : 0.4748 \r\n\r\n    mod6 <- train(classe ~.,method=\"rf\",data=train, trControl=trainControl(method=\"cv\"), number=3) # RF\r\n    pred6<-predict(mod6,test)\r\n    confusionMatrix(pred6, test$classe) # Accuracy : 0.9925  \r\n\r\n## Model which combine predictors.\r\n\r\n    predDF <- data.frame(pred1,pred2,pred3,pred4,pred5,pred6, classe=test$classe)\r\n    combModFit <- train(classe ~.,method=\"gbm\",data=predDF) # the new model\r\n    combPred <- predict(combModFit,newdata=predDF)\r\n    confusionMatrix(combPred, test$classe) # Accuracy : 0.9925 \r\n\r\nAs can be appreciated, the accuracy obtained after combining some models does not improve the accuracy obtained using Random Forest. That is the reason the model 6, which use Randon Forest, will be used instead of the combination of models.\r\n\r\n## Final prediction on the test set\r\n\r\n    pred6<-predict(mod6,test)\r\n\r\n    confusionMatrix(pred6, test$classe)\r\n    Confusion Matrix and Statistics\r\n\r\n          Reference\r\n    Prediction    A    B    C    D    E\r\n             A 1673   12    0    0    0\r\n             B    1 1126    2    0    1\r\n             C    0    1 1019   16    3\r\n             D    0    0    5  948    3\r\n             E    0    0    0    0 1075\r\n\r\n    Overall Statistics\r\n                                        \r\n                   Accuracy : 0.9925        \r\n                     95% CI : (0.99, 0.9946)\r\n        No Information Rate : 0.2845        \r\n        P-Value [Acc > NIR] : < 2.2e-16     \r\n                                        \r\n                      Kappa : 0.9905        \r\n     Mcnemar's Test P-Value : NA            \r\n\r\n    Statistics by Class:\r\n\r\n                         Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9994   0.9886   0.9932   0.9834   0.9935\r\n    Specificity            0.9972   0.9992   0.9959   0.9984   1.0000\r\n    Pos Pred Value         0.9929   0.9965   0.9808   0.9916   1.0000\r\n    Neg Pred Value         0.9998   0.9973   0.9986   0.9968   0.9985\r\n    Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n    Detection Rate         0.2843   0.1913   0.1732   0.1611   0.1827\r\n    Detection Prevalence   0.2863   0.1920   0.1766   0.1624   0.1827\r\n    Balanced Accuracy      0.9983   0.9939   0.9945   0.9909   0.9968\r\n\r\n\r\n## Final results\r\n\r\n    predict(mod6,testing) \r\n    B A B A A E D B A A B C B A E E A B B B\r\n    Levels: A B C D E","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}